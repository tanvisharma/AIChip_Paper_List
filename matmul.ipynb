{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanvisharma/AIChip_Paper_List/blob/master/matmul.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU0qsjfgvQ2S"
      },
      "source": [
        "# PyTorch Matrix Multiplication Demo\n",
        "In this notebook, we will compare several ways to perform matrix multiplication using PyTorch:\n",
        "\n",
        "1. **Naive Python for-loops** (for demonstration purposes, typically very slow)\n",
        "2. **torch.matmul()**\n",
        "3. **torch.mm()**\n",
        "4. **torch.bmm()** (batched matrix multiplication)\n",
        "5. **@ operator** (syntactic sugar for `torch.matmul`)\n",
        "6. **torch.einsum()**"
      ],
      "id": "DU0qsjfgvQ2S"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GW6AvFFvQ2U",
        "outputId": "7b94438b-5324-4f9b-f7f6-fcf863deae69"
      },
      "source": [
        "import torch\n",
        "import time\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.5.1+cu121\n"
          ]
        }
      ],
      "id": "4GW6AvFFvQ2U"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn_nL3HmvQ2V"
      },
      "source": [
        "## 1. Naive Matrix Multiplication (Python for loops)\n",
        "This is just to illustrate the concept. You wouldn't use this in practice in Python/PyTorch because it is much slower than built-in tensor operations."
      ],
      "id": "Hn_nL3HmvQ2V"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-7mni1AvQ2W",
        "outputId": "7f210ee0-ea8c-47b6-ebff-c7d1c37bbbb7"
      },
      "source": [
        "def naive_matrix_multiplication(A, B):\n",
        "    \"\"\"Compute matrix multiplication A x B using Python for loops.\"\"\"\n",
        "    # A: shape (n, k)\n",
        "    # B: shape (k, m)\n",
        "    n = A.shape[0]\n",
        "    k = A.shape[1]\n",
        "    m = B.shape[1]\n",
        "    C = torch.zeros(n, m, dtype=A.dtype)\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(m):\n",
        "            s = 0\n",
        "            for z in range(k):\n",
        "                s += A[i, z] * B[z, j]\n",
        "            C[i, j] = s\n",
        "\n",
        "    return C\n",
        "\n",
        "# Example sizes\n",
        "n, k, m = 4, 5, 3\n",
        "A = torch.randn(n, k)\n",
        "B = torch.randn(k, m)\n",
        "\n",
        "C_naive = naive_matrix_multiplication(A, B)\n",
        "print(\"Naive result\\n\", C_naive)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive result\n",
            " tensor([[ 7.2291, -1.0633,  1.8808],\n",
            "        [ 2.7907, -1.6227, -0.3606],\n",
            "        [-1.1496,  0.6015, -0.9736],\n",
            "        [ 1.6018, -0.3654,  0.5207]])\n"
          ]
        }
      ],
      "id": "9-7mni1AvQ2W"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w0oGJU6vQ2W"
      },
      "source": [
        "## 2. torch.matmul()\n",
        "`torch.matmul` is the most general-purpose matrix multiplication function in PyTorch. It can handle 1D, 2D, or higher-dimensional tensors (batch dimensions)."
      ],
      "id": "5w0oGJU6vQ2W"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuqer9N-vQ2W",
        "outputId": "cd22d138-a117-40b7-b57d-29aa75a97ce8"
      },
      "source": [
        "C_matmul = torch.matmul(A, B)\n",
        "print(\"torch.matmul result\\n\", C_matmul)\n",
        "\n",
        "# Let's compare with naive result\n",
        "difference = torch.sum(torch.abs(C_naive - C_matmul))\n",
        "print(\"Difference between naive and torch.matmul:\", difference.item())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.matmul result\n",
            " tensor([[ 7.2291, -1.0633,  1.8808],\n",
            "        [ 2.7907, -1.6227, -0.3606],\n",
            "        [-1.1496,  0.6015, -0.9736],\n",
            "        [ 1.6018, -0.3654,  0.5207]])\n",
            "Difference between naive and torch.matmul: 1.4007091522216797e-06\n"
          ]
        }
      ],
      "id": "cuqer9N-vQ2W"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPdNHxJIvQ2W"
      },
      "source": [
        "## 3. torch.mm()\n",
        "`torch.mm` only works for 2D matrices (no broadcasting, no batch dimensions). It's slightly more specialized than `torch.matmul`."
      ],
      "id": "zPdNHxJIvQ2W"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di7XVAxlvQ2W",
        "outputId": "418e36d2-d875-477c-fe8e-c2722610cad3"
      },
      "source": [
        "C_mm = torch.mm(A, B)\n",
        "print(\"torch.mm result\\n\", C_mm)\n",
        "\n",
        "difference_mm = torch.sum(torch.abs(C_mm - C_matmul))\n",
        "print(\"Difference between torch.mm and torch.matmul:\", difference_mm.item())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.mm result\n",
            " tensor([[ 7.2291, -1.0633,  1.8808],\n",
            "        [ 2.7907, -1.6227, -0.3606],\n",
            "        [-1.1496,  0.6015, -0.9736],\n",
            "        [ 1.6018, -0.3654,  0.5207]])\n",
            "Difference between torch.mm and torch.matmul: 0.0\n"
          ]
        }
      ],
      "id": "Di7XVAxlvQ2W"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gLTq-HvvQ2X"
      },
      "source": [
        "## 4. torch.bmm()\n",
        "`torch.bmm` performs a batch matrix-matrix product. It expects 3D tensors of the form `(batch_size, n, m)`.\n",
        "\n",
        "We'll create a batched version of our matrices and multiply them."
      ],
      "id": "3gLTq-HvvQ2X"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNjUnQmxvQ2X",
        "outputId": "e7ad6a6d-5b11-480d-e35f-6aadc4213640"
      },
      "source": [
        "batch_size = 2\n",
        "n, k, m = 3, 4, 5\n",
        "\n",
        "# Create batched tensors: shape: (batch_size, n, k), (batch_size, k, m)\n",
        "A_batched = torch.randn(batch_size, n, k)\n",
        "B_batched = torch.randn(batch_size, k, m)\n",
        "\n",
        "# bmm: (batch_size, n, k) x (batch_size, k, m) -> (batch_size, n, m)\n",
        "C_batched = torch.bmm(A_batched, B_batched)\n",
        "print(\"Shape of C_batched:\", C_batched.shape)\n",
        "print(\"C_batched\\n\", C_batched)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of C_batched: torch.Size([2, 3, 5])\n",
            "C_batched\n",
            " tensor([[[-1.3760,  1.9335, -0.0937, -2.3668, -4.4848],\n",
            "         [-0.5382, -1.2826,  1.7685, -1.5827,  4.2771],\n",
            "         [ 1.0298, -0.2790, -0.3781,  1.8991,  1.9159]],\n",
            "\n",
            "        [[ 0.0708, -1.3645,  0.0829,  0.9073,  1.0164],\n",
            "         [-1.6293, -2.5508, -3.6004, -1.9828,  0.8957],\n",
            "         [-0.9734, -3.0769,  1.3874,  3.3635, -0.1442]]])\n"
          ]
        }
      ],
      "id": "mNjUnQmxvQ2X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eba8JhclvQ2X"
      },
      "source": [
        "## 5. @ Operator\n",
        "The `@` operator in PyTorch is syntactic sugar for `torch.matmul()` when applied to PyTorch tensors."
      ],
      "id": "Eba8JhclvQ2X"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1VLr_x2vQ2X",
        "outputId": "33ed495c-1acc-4207-ad88-40dd189a1c4b"
      },
      "source": [
        "# We'll reuse A and B\n",
        "C_at = A @ B  # same as torch.matmul(A, B)\n",
        "print(\"@ operator result\\n\", C_at)\n",
        "\n",
        "difference_at = torch.sum(torch.abs(C_matmul - C_at))\n",
        "print(\"Difference between @ operator and torch.matmul:\", difference_at.item())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@ operator result\n",
            " tensor([[ 7.2291, -1.0633,  1.8808],\n",
            "        [ 2.7907, -1.6227, -0.3606],\n",
            "        [-1.1496,  0.6015, -0.9736],\n",
            "        [ 1.6018, -0.3654,  0.5207]])\n",
            "Difference between @ operator and torch.matmul: 0.0\n"
          ]
        }
      ],
      "id": "T1VLr_x2vQ2X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjjlRdYDvQ2X"
      },
      "source": [
        "## 6. torch.einsum()\n",
        "`torch.einsum` is very flexible. For a standard matrix multiplication of two 2D matrices, we can use the Einstein summation notation `ij,jk->ik`."
      ],
      "id": "sjjlRdYDvQ2X"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NMeZ0LOvQ2X",
        "outputId": "88fa5e05-c090-4654-a705-5e33d8502fda"
      },
      "source": [
        "C_einsum = torch.einsum('ij,jk->ik', A, B)\n",
        "print(\"torch.einsum result\\n\", C_einsum)\n",
        "\n",
        "difference_einsum = torch.sum(torch.abs(C_matmul - C_einsum))\n",
        "print(\"Difference between torch.matmul and torch.einsum:\", difference_einsum.item())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.einsum result\n",
            " tensor([[ 7.2291, -1.0633,  1.8808],\n",
            "        [ 2.7907, -1.6227, -0.3606],\n",
            "        [-1.1496,  0.6015, -0.9736],\n",
            "        [ 1.6018, -0.3654,  0.5207]])\n",
            "Difference between torch.matmul and torch.einsum: 1.4007091522216797e-06\n"
          ]
        }
      ],
      "id": "-NMeZ0LOvQ2X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D72V02EIvQ2X"
      },
      "source": [
        "## Performance Comparison\n",
        "We can do a quick time comparison using `%timeit` or simple timing."
      ],
      "id": "D72V02EIvQ2X"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVEf3H9evQ2X"
      },
      "source": [
        "# Let's do a larger matrix for timing tests\n",
        "n, k, m = 200, 200, 200\n",
        "A_large = torch.randn(n, k)\n",
        "B_large = torch.randn(k, m)\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [],
      "id": "VVEf3H9evQ2X"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Naive for loops (warning: can be slow for large sizes)\n",
        "start = time.time()\n",
        "C_naive_large = naive_matrix_multiplication(A_large, B_large)\n",
        "end = time.time()\n",
        "print(f\"Naive multiplication took: {end - start:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKBkFbMRw-PY",
        "outputId": "df4d0f8c-31f2-499b-9d0c-1ee1f3b389ea"
      },
      "id": "XKBkFbMRw-PY",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive multiplication took: 124.4101 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "executionInfo2": {},
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7C806xLvQ2X",
        "outputId": "f40e5de0-f2a1-4c2e-ef1d-9726fc2ed806"
      },
      "source": [
        "# 2. torch.matmul()\n",
        "start = time.time()\n",
        "C_matmul_large = torch.matmul(A_large, B_large)\n",
        "end = time.time()\n",
        "print(f\"torch.matmul took: {end - start:.6f} seconds\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.matmul took: 0.001682 seconds\n"
          ]
        }
      ],
      "id": "W7C806xLvQ2X"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs6kBt9QvQ2Y",
        "outputId": "8836d824-6d4b-4989-8d1a-70b329c15cf0"
      },
      "source": [
        "# 3. torch.mm()\n",
        "start = time.time()\n",
        "C_mm_large = torch.mm(A_large, B_large)\n",
        "end = time.time()\n",
        "print(f\"torch.mm took: {end - start:.6f} seconds\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.mm took: 0.001086 seconds\n"
          ]
        }
      ],
      "id": "Gs6kBt9QvQ2Y"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJj_SWuevQ2Y",
        "outputId": "b110baed-ae65-4120-e54a-b146e16c35df"
      },
      "source": [
        "# 4. @ operator\n",
        "start = time.time()\n",
        "C_at_large = A_large @ B_large\n",
        "end = time.time()\n",
        "print(f\"@ operator took: {end - start:.6f} seconds\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@ operator took: 0.001489 seconds\n"
          ]
        }
      ],
      "id": "NJj_SWuevQ2Y"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGApCDruvQ2Y",
        "outputId": "6e4b3d2d-06cc-45b5-8cb1-2c9a365399d6"
      },
      "source": [
        "# 5. torch.einsum()\n",
        "start = time.time()\n",
        "C_einsum_large = torch.einsum('ij,jk->ik', A_large, B_large)\n",
        "end = time.time()\n",
        "print(f\"torch.einsum took: {end - start:.6f} seconds\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.einsum took: 0.001968 seconds\n"
          ]
        }
      ],
      "id": "cGApCDruvQ2Y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J43K_FJWvQ2Y"
      },
      "source": [
        "We can see that naive for-loop implementation is significantly slower than any of the built-in tensor operations. Among the PyTorch operations (`matmul`, `mm`, `@`, `einsum`), you will typically find them close in performance, with potential small variations depending on backend optimizations."
      ],
      "id": "J43K_FJWvQ2Y"
    }
  ]
}